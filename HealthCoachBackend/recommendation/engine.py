# ============================================================
# Recommendation Engine â€” Running Coach
# ============================================================
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

from recommendation.loader import load_weeks, load_sessions
from recommendation.risk import compute_weekly_risk, risk_level_from_proba
from recommendation.clustering import (
    cluster_weeks,
    cluster_sessions,
)
from recommendation.schemas import WeekRecommendation

# ------------------------------------------------------------
# CONFIG
# ------------------------------------------------------------

CUTOFF_DATE = "2025-09-14"

SESSION_LABELS = {
    0: "intensity",
    1: "easy",
    2: "endurance",
}

FEATURES_WEEK = [
    "distance_km",
    "sessions",
    "duration_min",
    "weekly_load",
    "high_intensity_pct",
]

FEATURES_SESSION = [
    "distance_km",
    "duration_min",
    "pace_min_per_km",
    "low_intensity_pct",
    "high_intensity_pct",
]

# ============================================================
# Session type explanations â€” Running Coach
# ============================================================

SESSION_DESCRIPTIONS = {
    "easy": {
        "label": "SÃ©ance facile",
        "objective": "RÃ©cupÃ©ration active et maintien du volume",
        "description": (
            "Sortie courte Ã  allure confortable, rÃ©alisÃ©e majoritairement "
            "en basse intensitÃ©. La respiration doit rester facile et "
            "lâ€™effort ne doit pas gÃ©nÃ©rer de fatigue notable."
        ),
        "physiology": "Faible stress cardiovasculaire et musculaire",
        "data_profile": {
            "avg_duration_min": 38,
            "avg_distance_km": 5.7,
            "low_intensity_pct": 0.92,
        },
    },
    "endurance": {
        "label": "SÃ©ance dâ€™endurance",
        "objective": "DÃ©veloppement de lâ€™endurance aÃ©robie",
        "description": (
            "Sortie plus longue Ã  allure modÃ©rÃ©e, majoritairement en basse intensitÃ©. "
            "Lâ€™objectif est de construire une base aÃ©robie solide et "
            "dâ€™amÃ©liorer la capacitÃ© Ã  maintenir un effort dans le temps."
        ),
        "physiology": "Sollicitation aÃ©robie prolongÃ©e, fatigue progressive",
        "data_profile": {
            "avg_duration_min": 76,
            "avg_distance_km": 11.8,
            "low_intensity_pct": 0.91,
        },
    },
    "intensity": {
        "label": "SÃ©ance intense",
        "objective": "AmÃ©lioration de la vitesse et de la capacitÃ© cardiovasculaire",
        "description": (
            "SÃ©ance structurÃ©e avec une part importante dâ€™efforts Ã  haute intensitÃ© "
            "(fractionnÃ©, tempo, blocs soutenus). "
            "Cette sÃ©ance est exigeante et nÃ©cessite une rÃ©cupÃ©ration adÃ©quate."
        ),
        "physiology": "Stress cardiovasculaire et neuromusculaire Ã©levÃ©",
        "data_profile": {
            "avg_duration_min": 66,
            "avg_distance_km": 11.1,
            "high_intensity_pct": 0.56,
        },
    },
}

SESSION_PROFILES = {
    "easy": {
        "label": "SÃ©ance facile",
        "data_profile": {
            "avg_duration_min": 38,
            "avg_distance_km": 5.7,
            "low_intensity_pct": 0.92,
            "high_intensity_pct": 0.08,
        },
    },
    "endurance": {
        "label": "SÃ©ance dâ€™endurance",
        "data_profile": {
            "avg_duration_min": 76,
            "avg_distance_km": 11.8,
            "low_intensity_pct": 0.91,
            "high_intensity_pct": 0.09,
        },
    },
    "intensity": {
        "label": "SÃ©ance intense",
        "data_profile": {
            "avg_duration_min": 66,
            "avg_distance_km": 11.1,
            "low_intensity_pct": 0.44,
            "high_intensity_pct": 0.56,
        },
    },
}

# ------------------------------------------------------------
# MAIN ENTRY POINT
# ------------------------------------------------------------


def compute_week_recommendation_from_csv(
    weeks_path: str = "weeks_received.csv",
    sessions_path: str = "sessions_received.csv",
) -> WeekRecommendation:
    # --------------------------------------------------------
    # 1. LOAD DATA
    # --------------------------------------------------------

    weeks = load_weeks()
    sessions = load_sessions()

    # --------------------------------------------------------
    # 2. CLUSTER WEEKS
    # --------------------------------------------------------

    weeks_clustered = cluster_weeks(weeks)

    # --------------------------------------------------------
    # 3. CLUSTER SESSIONS
    # --------------------------------------------------------

    sessions_clustered = cluster_sessions(sessions)

    # --------------------------------------------------------
    # 4. SESSION DISTRIBUTION PER WEEK CLUSTER
    # --------------------------------------------------------

    distribution = _compute_session_distribution(sessions_clustered, weeks_clustered)

    # --------------------------------------------------------
    # 5. RISK SCORE (via risk.py)
    # --------------------------------------------------------

    risk_df = compute_weekly_risk(weeks_clustered)

    # --------------------------------------------------------
    # 6. RECENT DYNAMICS
    # --------------------------------------------------------

    current_week_start = weeks_clustered["week_start"].max()
    completed_weeks = weeks_clustered[
        weeks_clustered["week_start"] < current_week_start
    ]

    last_3w = completed_weeks.tail(3)

    dominant_cluster = int(last_3w["cluster_week"].mode()[0])
    avg_sessions = int(np.clip(round(last_3w["sessions"].mean()), 2, 5))

    avg_risk = risk_df[risk_df["week_start"].isin(last_3w["week_start"])][
        "risk_proba"
    ].mean()

    risk_level = risk_level_from_proba(avg_risk)

    # --------------------------------------------------------
    # 7. BUILD PLAN
    # --------------------------------------------------------

    base_plan = _build_week_template(dominant_cluster, avg_sessions, distribution)

    base_plan = _adjust_plan_by_risk(base_plan, avg_risk)

    current_sessions = sessions_clustered[
        sessions_clustered["week_start"] == current_week_start
    ]

    done_types = current_sessions["cluster_session"].map(SESSION_LABELS).tolist()
    done_sessions_summary = summarize_done_sessions(current_sessions)

    print("\nðŸ§© Mapping cluster_session â†’ label :")
    for k, v in SESSION_LABELS.items():
        print(f"cluster {k} â†’ {v}")

    remaining_sessions = max(0, avg_sessions - len(done_types))

    final_plan = _adjust_with_done_sessions(base_plan, done_types, remaining_sessions)

    enriched_remaining_plan = enrich_plan_with_descriptions(final_plan)

    # --------------------------------------------------------
    # 8. OUTPUT
    # --------------------------------------------------------
    print("\n================ DEBUG SEMAINE EN COURS ================")
    print("ðŸ“… Semaine en cours :", current_week_start)

    print("\nðŸ“¥ SÃ©ances dÃ©tectÃ©es cette semaine :")
    print(
        current_sessions[
            [
                "date",
                "distance_km",
                "duration_min",
                "high_intensity_pct",
                "cluster_session",
            ]
        ]
    )

    print(done_sessions_summary)

    print("\nðŸ·ï¸ done_types (aprÃ¨s mapping cluster â†’ label) :")
    print(done_types)

    print("\n================ DEBUG RECOMMENDATION ================")
    print("ðŸ“… Current week start:", current_week_start)

    print("\nðŸ“¥ Sessions de la semaine en cours :")
    print(
        current_sessions[
            [
                "date",
                "distance_km",
                "duration_min",
                "high_intensity_pct",
                "cluster_session",
            ]
        ]
    )

    print("\nðŸ·ï¸ Types de sÃ©ances dÃ©tectÃ©s (cluster â†’ label) :")
    print(done_types)

    return {
        "target_sessions": avg_sessions,
        "dominant_week_cluster": dominant_cluster,
        "avg_risk_last_3w": round(float(avg_risk), 2),
        "risk_level": risk_level,
        "base_plan": base_plan,
        "adjusted_plan_remaining": final_plan,
        "done_sessions": done_types,
        "remaining_sessions": enriched_remaining_plan,
        "done_sessions_details": done_sessions_summary,
    }


# ------------------------------------------------------------
# INTERNAL HELPERS (unchanged)
# ------------------------------------------------------------


def _cluster_weeks(df):
    df = df.dropna(subset=FEATURES_WEEK).copy()
    X = StandardScaler().fit_transform(df[FEATURES_WEEK])
    df["cluster_week"] = KMeans(n_clusters=3, random_state=42, n_init=20).fit_predict(X)
    return df


def _cluster_sessions(df):
    df = df.dropna(subset=FEATURES_SESSION).copy()
    X = StandardScaler().fit_transform(df[FEATURES_SESSION])
    df["cluster_session"] = KMeans(
        n_clusters=3, random_state=42, n_init=20
    ).fit_predict(X)
    return df


def summarize_done_sessions(current_sessions: pd.DataFrame) -> list[dict]:
    summaries = []

    for _, row in current_sessions.iterrows():
        summaries.append(
            {
                "type": SESSION_LABELS[row["cluster_session"]],
                "duration_min": round(row["duration_min"], 1),
                "distance_km": round(row["distance_km"], 1),
                "low_intensity_pct": round(row["low_intensity_pct"], 2),
                "high_intensity_pct": round(row["high_intensity_pct"], 2),
            }
        )

    return summaries


def _compute_session_distribution(sessions, weeks):
    counts = (
        sessions.groupby(["week_start", "cluster_session"])
        .size()
        .reset_index(name="n_sessions")
    )

    counts = counts.merge(
        weeks[["week_start", "cluster_week"]],
        on="week_start",
        how="left",
    )

    dist = (
        counts.groupby(["cluster_week", "cluster_session"])["n_sessions"]
        .mean()
        .reset_index()
    )

    dist["pct"] = dist.groupby("cluster_week")["n_sessions"].transform(
        lambda x: x / x.sum()
    )

    return dist


def _compute_risk_scores(df):
    df = df.dropna(subset=FEATURES_WEEK).copy()
    load_threshold = df["weekly_load"].quantile(0.80)

    def is_risky(row):
        return int(
            row["cluster_week"] == 1
            or row["weekly_load"] > load_threshold
            or row["high_intensity_pct"] > 0.45
        )

    df["risk"] = df.apply(is_risky, axis=1)

    X = df[FEATURES_WEEK]
    y = df["risk"]

    model = Pipeline(
        [
            ("scaler", StandardScaler()),
            ("model", LogisticRegression()),
        ]
    )

    model.fit(X, y)
    df["risk_proba"] = model.predict_proba(X)[:, 1]

    return df


def _build_week_template(cluster_week, avg_sessions, dist_df):
    rows = dist_df[dist_df["cluster_week"] == cluster_week]
    plan = []

    for _, row in rows.iterrows():
        plan += [SESSION_LABELS[row["cluster_session"]]] * round(
            row["pct"] * avg_sessions
        )

    while len(plan) < avg_sessions:
        plan.append("easy")
    while len(plan) > avg_sessions:
        plan.pop()

    return plan


def _adjust_plan_by_risk(plan, avg_risk):
    if avg_risk > 0.75:
        return ["easy" if s == "intensity" else s for s in plan]
    if avg_risk < 0.30 and "intensity" not in plan:
        plan[-1] = "intensity"
    return plan


def _adjust_with_done_sessions(plan, done_types, remaining_sessions):
    print("\nðŸ› ï¸ DEBUG _adjust_with_done_sessions")
    print("Plan initial :", plan)
    print("SÃ©ances dÃ©jÃ  faites :", done_types)
    print("Nombre de sÃ©ances restantes :", remaining_sessions)

    plan_copy = plan.copy()

    for done in done_types:
        if done in plan_copy:
            print(f"âž¡ï¸ Suppression de : {done}")
            plan_copy.remove(done)
        else:
            print(f"âš ï¸ {done} non trouvÃ© dans le plan")

    adjusted = plan_copy[:remaining_sessions]

    while len(adjusted) < remaining_sessions:
        adjusted.append("easy")

    print("Plan final ajustÃ© :", adjusted)
    return adjusted


def enrich_plan_with_descriptions(plan: list[str]) -> list[dict]:
    """
    Convert a list of session types into rich session recommendations.
    """
    enriched = []

    for session_type in plan:
        # desc = SESSION_DESCRIPTIONS.get(session_type)
        profile = SESSION_PROFILES.get(session_type)

        if profile is None:
            continue

        enriched.append(
            {
                "type": session_type,
                "label": profile["label"],
                # "objective": profile["objective"],
                # "description": profile["description"],
                # "physiology": profile["physiology"],
                "data_profile": profile["data_profile"],
            }
        )

    return enriched
